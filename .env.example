# Environment Configuration for Agentic Workflow

# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434

# Model Selection (options: mistral, neural-chat, orca-mini, llama2)
MODEL_NAME=mistral

# GPU Configuration
DEVICE=cuda

# Logging Level (DEBUG, INFO, WARNING, ERROR)
LOG_LEVEL=INFO
